{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ddc97f7-8802-4b84-8845-99df184209b6",
   "metadata": {},
   "source": [
    "#Â Naive Bayes algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "32afc343-18de-4cbd-a24a-d451ff17cc52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T10:10:30.069415Z",
     "start_time": "2025-07-24T10:10:28.609226Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9733333333333334\n",
      "Test Accuracy: 0.9466666666666667\n",
      "Number of mislabeled points out of a total 75 points : 4\n"
     ]
    }
   ],
   "source": [
    "# Gaussian Naive Bayes for Iris Flowers\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)\n",
    "\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "print('Training Accuracy:',accuracy_score(y_pred_train,y_train))\n",
    "print('Test Accuracy:',accuracy_score(y_pred_test,y_test))\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (X_test.shape[0], (y_test != y_pred_test).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7df7ba42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9699570815450643\n",
      "Test Accuracy: 0.9603960396039604\n",
      "Number of mislabeled points out of a total 101 points : 4\n"
     ]
    }
   ],
   "source": [
    "# --->>> Your turn <<<---\n",
    "# apply Gaussian Naive Bayes to the penguins dataset\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn import neighbors\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "    \n",
    "n_neighbors = 15\n",
    "\n",
    "# import some data to play with\n",
    "penguins = pd.read_csv('penguins_size.csv').dropna()\n",
    "X = penguins[['culmen_length_mm', 'culmen_depth_mm','flipper_length_mm','body_mass_g']].values\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(penguins['species'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "print('Training Accuracy:',accuracy_score(y_pred_train,y_train))\n",
    "print('Test Accuracy:',accuracy_score(y_pred_test,y_test))\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (X_test.shape[0], (y_test != y_pred_test).sum()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e5f7f4a4-2d31-4825-97f0-83cd6d1f2d0b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T10:10:30.101185Z",
     "start_time": "2025-07-24T10:10:30.094992Z"
    }
   },
   "outputs": [],
   "source": [
    "# --->>> Your turn <<<---\n",
    "# Implement a Naive Bayes calculation from scratch for the following problem,\n",
    "# then compare your result with the sklearn result\n",
    "#\n",
    "# Proposed approach: \n",
    "# calculate the probabilities of different words (the features), and\n",
    "# use the naive bayes formula to derive the probability for classes 0 and 1\n",
    "#\n",
    "# Hint: for the sklearn approach you can use MultinomialNB, and CountVectorizer\n",
    "# \n",
    "# Given the three sentences  \n",
    "sentence1 = \"This is the first sentence in English\"\n",
    "sentence2 = \"Another sentence also in English\"\n",
    "sentence3 = \"This is not a sentence\"\n",
    "# which are assigned the classes \n",
    "y = [0,0,1]\n",
    "\n",
    "# Use Naive Bayes to estimate the class of \n",
    "test_sentence = \"This not English\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "68c2a714-5440-45ba-a294-06f6bb089530",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T10:10:30.361740Z",
     "start_time": "2025-07-24T10:10:30.352358Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a priori probabilities: {0: 0.6666666666666666, 1: 0.3333333333333333}\n",
      "class probabilities {0: 0.001185185185185185, 1: 0.0026041666666666665}\n",
      "normalised class probabilities 0.31276725717776416 0.6872327428222358\n",
      "predicted class: 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "docs, p = {}, {}\n",
    "docs[0] = sentence1 + \" \" + sentence2 \n",
    "docs[1] = sentence3 \n",
    "alpha = 1.0\n",
    "\n",
    "lclasses = np.unique(y)\n",
    "for c in lclasses:\n",
    "    docs[c] = docs[c].split()\n",
    "    p[c] = y.count(c)/len(y)\n",
    "print(\"a priori probabilities:\", p)\n",
    "\n",
    "features = test_sentence.split()\n",
    "for xi in features:\n",
    "    for c in np.unique(y):\n",
    "        p[c] *= (docs[c].count(xi) + alpha) / (len(docs[c]) + alpha*len(features))\n",
    "\n",
    "print(\"class probabilities\", p)\n",
    "print(\"normalised class probabilities\", p[0]/(p[0]+p[1]), p[1]/(p[0]+p[1]))\n",
    "p = np.array(list(p.values()))\n",
    "print(\"predicted class:\",np.argmax(p))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2e902ab4-42d4-42aa-b74b-3f50bea2c40b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T10:10:30.402146Z",
     "start_time": "2025-07-24T10:10:30.392130Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['also' 'another' 'english' 'first' 'in' 'is' 'not' 'sentence' 'the'\n",
      " 'this']\n"
     ]
    }
   ],
   "source": [
    "# Now use sklearn to verify your result\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "X_train = [sentence1, sentence2, sentence3] \n",
    "X_test = [test_sentence] \n",
    "\n",
    "vectorizer = CountVectorizer(stop_words=None)\n",
    "X_train = vectorizer.fit_transform(X_train).toarray()\n",
    "print( vectorizer.get_feature_names_out() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "70366b11-547d-4ac4-8e63-66c65d2411d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T10:10:30.418134Z",
     "start_time": "2025-07-24T10:10:30.413185Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 1 1 1 0 1 1 1]\n",
      " [1 1 1 0 1 0 0 1 0 0]\n",
      " [0 0 0 0 0 1 1 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# Let's see the encoding of our three sentences\n",
    "# the columns are the counts of the words above\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7971eebd-245e-4ccc-9d9f-b3c641d32a4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T10:10:30.466476Z",
     "start_time": "2025-07-24T10:10:30.455956Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 0, 0, 1, 0, 0, 1]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.transform(X_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e549d81-fba7-47ae-8217-4933330e016b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T10:10:30.562851Z",
     "start_time": "2025-07-24T10:10:30.551343Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "[[0.43601695 0.56398305]]\n"
     ]
    }
   ],
   "source": [
    "mnnb = MultinomialNB()\n",
    "mnnb.fit(X_train, y)\n",
    "\n",
    "X_test2 = vectorizer.transform(X_test).toarray()\n",
    "print(mnnb.predict(X_test2))\n",
    "#print probabilities for each class\n",
    "print(mnnb.predict_proba(X_test2))\n",
    "#Comment: CountVectorizer removes single letter words like 'a' by default, so this changes the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5daa36ef-366f-4b87-a8f6-46e90b939271",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T10:10:30.730404Z",
     "start_time": "2025-07-24T10:10:30.726906Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "[[0.1445245 0.8554755]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import CategoricalNB\n",
    "cnb = CategoricalNB()\n",
    "cnb.fit(X_train, y)\n",
    "\n",
    "X_test2 = vectorizer.transform(X_test).toarray()\n",
    "print(cnb.predict(X_test2))\n",
    "#print probabilities for each class\n",
    "print(cnb.predict_proba(X_test2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1778e6b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLCO2_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
